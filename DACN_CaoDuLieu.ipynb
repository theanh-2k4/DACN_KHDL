{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cd512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c09b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube_scraper.py\n",
    "\n",
    "\n",
    "# ========== CẤU HÌNH ==========\n",
    "API_KEY = \"YOUR_YOUTUBE_API_KEY\"   # <-- thay bằng API key của bạn\n",
    "YOUTUBE = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "QUERY = \"iPhone 15 review\"        # từ khóa / campaign / product để tìm video\n",
    "MAX_VIDEOS = 200                  # số video tối đa thu thập\n",
    "MAX_COMMENTS_PER_VIDEO = 200      # số comments tối đa/ video (API giới hạn 100 per page)\n",
    "OUTPUT_CSV = \"youtube_comments.csv\"\n",
    "OUTPUT_JSON = \"youtube_comments.json\"\n",
    "SLEEP_BETWEEN_REQUESTS = 0.1      # tránh quá nhanh dễ dính quota/limit\n",
    "\n",
    "# ========== HÀM HỖ TRỢ ==========\n",
    "def search_videos(query, max_results=100):\n",
    "    \"\"\"Tìm video theo query, trả về list video dict (videoId, title, publishedAt).\"\"\"\n",
    "    videos = []\n",
    "    nextPageToken = None\n",
    "    fetched = 0\n",
    "    while fetched < max_results:\n",
    "        to_fetch = min(50, max_results - fetched)  # search.list maxResults ≤ 50\n",
    "        res = YOUTUBE.search().list(\n",
    "            q=query,\n",
    "            part=\"id,snippet\",\n",
    "            type=\"video\",\n",
    "            maxResults=to_fetch,\n",
    "            pageToken=nextPageToken\n",
    "        ).execute()\n",
    "        for item in res.get(\"items\", []):\n",
    "            vid = item[\"id\"][\"videoId\"]\n",
    "            title = item[\"snippet\"][\"title\"]\n",
    "            publ = item[\"snippet\"][\"publishedAt\"]\n",
    "            videos.append({\"videoId\": vid, \"title\": title, \"publishedAt\": publ})\n",
    "        fetched += len(res.get(\"items\", []))\n",
    "        nextPageToken = res.get(\"nextPageToken\")\n",
    "        if not nextPageToken:\n",
    "            break\n",
    "        time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "    return videos\n",
    "\n",
    "def fetch_comments_for_video(video_id, max_comments=200):\n",
    "    \"\"\"Lấy commentThreads (top-level comments) cho 1 video, kèm replies_count.\n",
    "       Trả về list comment dicts.\"\"\"\n",
    "    comments = []\n",
    "    nextPageToken = None\n",
    "    fetched = 0\n",
    "    while fetched < max_comments:\n",
    "        to_fetch = min(100, max_comments - fetched)  # commentThreads maxResults ≤ 100\n",
    "        res = YOUTUBE.commentThreads().list(\n",
    "            part=\"id,snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=to_fetch,\n",
    "            pageToken=nextPageToken,\n",
    "            textFormat=\"plainText\"   # lấy raw text\n",
    "        ).execute()\n",
    "        for item in res.get(\"items\", []):\n",
    "            top = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            c = {\n",
    "                \"videoId\": video_id,\n",
    "                \"commentId\": item[\"id\"],\n",
    "                \"authorDisplayName\": top.get(\"authorDisplayName\"),\n",
    "                \"authorChannelId\": top.get(\"authorChannelId\", {}).get(\"value\"),\n",
    "                \"textOriginal\": top.get(\"textOriginal\"),\n",
    "                \"likeCount\": top.get(\"likeCount\"),\n",
    "                \"publishedAt\": top.get(\"publishedAt\"),\n",
    "                \"updatedAt\": top.get(\"updatedAt\"),\n",
    "                \"replyCount\": item[\"snippet\"].get(\"totalReplyCount\", 0)\n",
    "            }\n",
    "            comments.append(c)\n",
    "            fetched += 1\n",
    "        nextPageToken = res.get(\"nextPageToken\")\n",
    "        if not nextPageToken:\n",
    "            break\n",
    "        time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "    return comments\n",
    "\n",
    "# ========== CHẠY THU THẬP ==========\n",
    "def main():\n",
    "    print(\"Searching videos for query:\", QUERY)\n",
    "    videos = search_videos(QUERY, max_results=MAX_VIDEOS)\n",
    "    print(f\"Found {len(videos)} videos. Start fetching comments...\")\n",
    "\n",
    "    all_comments = []\n",
    "    for v in tqdm(videos, desc=\"Videos\"):\n",
    "        vid = v[\"videoId\"]\n",
    "        title = v[\"title\"]\n",
    "        publ = v[\"publishedAt\"]\n",
    "        # fetch top-level comments\n",
    "        try:\n",
    "            comments = fetch_comments_for_video(vid, max_comments=MAX_COMMENTS_PER_VIDEO)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments for video {vid}: {e}\")\n",
    "            comments = []\n",
    "        # bổ sung metadata video vào từng comment\n",
    "        for c in comments:\n",
    "            c[\"videoTitle\"] = title\n",
    "            c[\"videoPublishedAt\"] = publ\n",
    "        all_comments.extend(comments)\n",
    "        time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "\n",
    "    # lưu CSV\n",
    "    if all_comments:\n",
    "        df = pd.DataFrame(all_comments)\n",
    "        df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        # lưu JSON\n",
    "        with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_comments, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved {len(all_comments)} comments to {OUTPUT_CSV} and {OUTPUT_JSON}\")\n",
    "    else:\n",
    "        print(\"No comments collected.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
